from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain
from textwrap import fill

# Load PDF
file_path = "hacking_prices.pdf"
loader = PyPDFLoader(file_path)
docs = loader.load()

# Define LLM for Mistral-Nemo
llm = ChatOpenAI(
    temperature=0.1,
    model_name="mistral-nemo:latest",
    api_key="your_api_key",  # Replace with the key or environment variable
    base_url="http://localhost:11434/v1",  # Adjust to your Ollama server URL
)

# Prompt for Summarization
summary_prompt_template = """Write a long summary of the following document. 
Only include information that is part of the document. 
Do not include your own opinion or analysis.

Document:
"{document}"
Summary:"""
summary_prompt = PromptTemplate.from_template(summary_prompt_template)
summary_chain = StuffDocumentsChain(
    llm_chain=LLMChain(llm=llm, prompt=summary_prompt),
    document_variable_name="document"
)

# Summarize Document
def summarize_docs(docs):
    try:
        result = summary_chain.invoke(docs)
        return fill(result["output_text"])
    except Exception as e:
        return f"Error during summarization: {e}"

# Query Document with Map-Reduce
def query_docs(docs, user_query):
    try:
        # Map and Reduce Prompts
        map_template = """The following is a set of documents:
        {docs}
        Based on this list of documents, please identify the information that is most relevant to the following query:
        {user_query}
        If the document is not relevant, please write "not relevant."
        Helpful Answer:"""
        reduce_template = """The following is a set of partial answers to a user query:
        {docs}
        Take these and distill them into a final, consolidated answer to the following query:
        {user_query}
        Complete Answer:"""

        # Map and Reduce Chains
        map_prompt = PromptTemplate.from_template(map_template).partial(user_query=user_query)
        reduce_prompt = PromptTemplate.from_template(reduce_template).partial(user_query=user_query)

        map_chain = LLMChain(llm=llm, prompt=map_prompt)
        reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)

        combine_documents_chain = StuffDocumentsChain(
            llm_chain=reduce_chain, document_variable_name="docs"
        )
        reduce_documents_chain = ReduceDocumentsChain(
            combine_documents_chain=combine_documents_chain,
            collapse_documents_chain=combine_documents_chain,
            token_max=4000,
        )

        # Map-Reduce Chain
        map_reduce_chain = MapReduceDocumentsChain(
            llm_chain=map_chain,
            reduce_documents_chain=reduce_documents_chain,
            document_variable_name="docs",
            return_intermediate_steps=False,
        )
        result = map_reduce_chain.invoke(docs)
        return fill(result["output_text"])
    except Exception as e:
        return f"Error during query: {e}"

# Example Usage
if __name__ == "__main__":
    print("### Summarization ###")
    print(summarize_docs(docs))
    print("\n### Query ###")
    user_query = "What is the data used in this analysis?"
    print(query_docs(docs[:-3], user_query))
